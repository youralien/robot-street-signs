{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Recognizer CNN\n",
    "\n",
    "This notebook will create the sign recognizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import text\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import glob, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows, img_cols = 96, 128\n",
    "in_shape = (img_rows, img_cols, 3)\n",
    "batch_size = 256\n",
    "nb_classes = 2\n",
    "nb_epoch = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/home/nathan/olin/spring2017/sign_follower/scripts/data/train2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "                rotation_range=10,\n",
    "                width_shift_range=0.02,\n",
    "                height_shift_range=0.02,\n",
    "                rescale=1./255,\n",
    "                shear_range=0.2,\n",
    "                zoom_range=0.2,\n",
    "                vertical_flip = False,\n",
    "                horizontal_flip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31382 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '/home/nathan/olin/spring2017/sign_follower/scripts/data/train2',  # this is the target directory\n",
    "        target_size=(img_rows, img_cols),  # all images will be resized to 96x128\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'room': 0, 'sign': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add convolutional layers\n",
    "\n",
    "Nothing special, just 3 sequential 64 3x3 conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Convolution2D(64, 3, 3, border_mode='valid', input_shape=in_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add fully connected layers\n",
    "\n",
    "Single fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(192))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5)) # helps prevent overfitting\n",
    "\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "256/256 [==============================] - 2s - loss: 0.7332 - acc: 0.0664\n",
      "Epoch 2/1500\n",
      "256/256 [==============================] - 0s - loss: 0.2507 - acc: 0.9805\n",
      "Epoch 3/1500\n",
      "256/256 [==============================] - 0s - loss: 0.0696 - acc: 0.9961\n",
      "Epoch 4/1500\n",
      "256/256 [==============================] - 0s - loss: 0.3744 - acc: 0.9766\n",
      "Epoch 5/1500\n",
      "256/256 [==============================] - 0s - loss: 0.3183 - acc: 0.9805\n",
      "Epoch 6/1500\n",
      "256/256 [==============================] - 0s - loss: 0.0639 - acc: 0.9961\n",
      "Epoch 7/1500\n",
      "256/256 [==============================] - 0s - loss: 0.2519 - acc: 0.9844\n",
      "Epoch 8/1500\n",
      "256/256 [==============================] - 0s - loss: 0.1889 - acc: 0.9883\n",
      "Epoch 9/1500\n",
      "256/256 [==============================] - 0s - loss: 0.4407 - acc: 0.9727\n",
      "Epoch 10/1500\n",
      "256/256 [==============================] - 0s - loss: 0.2518 - acc: 0.9844\n",
      "Epoch 11/1500\n",
      "256/256 [==============================] - 0s - loss: 0.4407 - acc: 0.9727\n",
      "Epoch 12/1500\n",
      "256/256 [==============================] - 0s - loss: 0.3148 - acc: 0.9805\n",
      "Epoch 13/1500\n",
      "256/256 [==============================] - 0s - loss: 0.1259 - acc: 0.9922\n",
      "Epoch 14/1500\n",
      "256/256 [==============================] - 0s - loss: 0.1889 - acc: 0.9883\n",
      "Epoch 15/1500\n",
      "256/256 [==============================] - 0s - loss: 0.1889 - acc: 0.9883\n",
      "Epoch 16/1500\n",
      "256/256 [==============================] - 0s - loss: 0.1889 - acc: 0.9883\n",
      "Epoch 17/1500\n",
      "256/256 [==============================] - 0s - loss: 0.3778 - acc: 0.9766\n",
      "Epoch 18/1500\n",
      "256/256 [==============================] - 0s - loss: 0.3148 - acc: 0.9805\n",
      "Epoch 19/1500\n",
      "256/256 [==============================] - 0s - loss: 0.1889 - acc: 0.9883\n",
      "Epoch 20/1500\n",
      "256/256 [==============================] - 0s - loss: 0.2518 - acc: 0.9844\n",
      "Epoch 21/1500\n",
      "256/256 [==============================] - 0s - loss: 0.3778 - acc: 0.9766\n",
      "Epoch 22/1500\n",
      "256/256 [==============================] - 0s - loss: 0.3148 - acc: 0.9805\n",
      "Epoch 23/1500\n",
      "256/256 [==============================] - 0s - loss: 0.5667 - acc: 0.9648\n",
      "Epoch 24/1500\n",
      "256/256 [==============================] - 0s - loss: 0.2518 - acc: 0.9844\n",
      "Epoch 25/1500\n",
      "256/256 [==============================] - 0s - loss: 0.3148 - acc: 0.9805\n",
      "Epoch 26/1500\n",
      "256/256 [==============================] - 0s - loss: 0.5667 - acc: 0.9648\n",
      "Epoch 27/1500\n",
      "256/256 [==============================] - 0s - loss: 0.3148 - acc: 0.9805\n",
      "Epoch 28/1500\n",
      "256/256 [==============================] - 0s - loss: 0.3148 - acc: 0.9805\n",
      "Epoch 29/1500\n",
      "256/256 [==============================] - 0s - loss: 0.3778 - acc: 0.9766\n",
      "Epoch 30/1500\n",
      "256/256 [==============================] - 0s - loss: 0.2518 - acc: 0.9844\n",
      "Epoch 31/1500\n",
      "256/256 [==============================] - 0s - loss: 0.2518 - acc: 0.9844\n",
      "Epoch 32/1500\n",
      "256/256 [==============================] - 0s - loss: 0.0630 - acc: 0.9961\n",
      "Epoch 33/1500\n",
      "256/256 [==============================] - 0s - loss: 0.5667 - acc: 0.9648\n",
      "Epoch 34/1500\n",
      "256/256 [==============================] - 0s - loss: 0.5037 - acc: 0.9688\n",
      "Epoch 35/1500\n",
      "256/256 [==============================] - 0s - loss: 0.3148 - acc: 0.9805\n",
      "Epoch 36/1500\n",
      "256/256 [==============================] - 0s - loss: 0.3148 - acc: 0.9805\n",
      "Epoch 37/1500\n",
      "256/256 [==============================] - 0s - loss: 0.1259 - acc: 0.9922\n",
      "Epoch 38/1500\n",
      "256/256 [==============================] - 0s - loss: 0.3148 - acc: 0.9805\n",
      "Epoch 39/1500\n",
      "256/256 [==============================] - 0s - loss: 0.1259 - acc: 0.9922\n",
      "Epoch 40/1500\n",
      "256/256 [==============================] - 0s - loss: 0.1259 - acc: 0.9922\n",
      "Epoch 41/1500\n",
      "256/256 [==============================] - 0s - loss: 0.3778 - acc: 0.9766\n",
      "Epoch 42/1500\n",
      "256/256 [==============================] - 0s - loss: 0.5037 - acc: 0.9688\n",
      "Epoch 43/1500\n",
      "256/256 [==============================] - 0s - loss: 0.4407 - acc: 0.9727\n",
      "Epoch 44/1500\n",
      "256/256 [==============================] - 0s - loss: 0.1889 - acc: 0.9883\n",
      "Epoch 45/1500\n",
      "256/256 [==============================] - 0s - loss: 0.3148 - acc: 0.9805\n",
      "Epoch 46/1500\n",
      "256/256 [==============================] - 0s - loss: 0.4407 - acc: 0.9727\n",
      "Epoch 47/1500\n",
      "256/256 [==============================] - 0s - loss: 0.1889 - acc: 0.9883\n",
      "Epoch 48/1500\n",
      "256/256 [==============================] - 0s - loss: 0.1889 - acc: 0.9883\n",
      "Epoch 49/1500\n",
      "256/256 [==============================] - 0s - loss: 0.3778 - acc: 0.9766\n",
      "Epoch 50/1500\n",
      "256/256 [==============================] - 0s - loss: 0.2518 - acc: 0.9844\n",
      "Epoch 51/1500\n",
      "256/256 [==============================] - 0s - loss: 0.3148 - acc: 0.9805\n",
      "Epoch 52/1500\n",
      "256/256 [==============================] - 0s - loss: 0.5667 - acc: 0.9648\n",
      "Epoch 53/1500\n",
      "256/256 [==============================] - 0s - loss: 0.2518 - acc: 0.9844\n",
      "Epoch 54/1500\n",
      "256/256 [==============================] - 0s - loss: 0.1889 - acc: 0.9883\n",
      "Epoch 55/1500\n",
      "256/256 [==============================] - 0s - loss: 0.6296 - acc: 0.9609\n",
      "Epoch 56/1500\n",
      "256/256 [==============================] - 0s - loss: 0.3148 - acc: 0.9805\n",
      "Epoch 57/1500\n",
      "256/256 [==============================] - 0s - loss: 0.1259 - acc: 0.9922\n",
      "Epoch 58/1500\n",
      "256/256 [==============================] - 0s - loss: 0.4407 - acc: 0.9727\n",
      "Epoch 59/1500\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=batch_size,\n",
    "        nb_epoch=nb_epoch,\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old weights:  \n",
    "1_maybeGotLucky.h5  \n",
    "2_betterDataset.h5  \n",
    "3_removedFlips.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('3_removedFlips.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('3_removedFlips.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/home/nathan/olin/spring2017/sign_follower/scripts/data/valid2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = np.asarray([np.asarray(Image.open(file)) for file in glob.glob(\"*.png\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for file in tqdm(glob.glob(\"*.png\")):\n",
    "#     img  = Image.open(file)\n",
    "#     data = np.asarray(img)\n",
    "#     try:\n",
    "#         test_data = np.concatenate((test_data, data), axis=0)\n",
    "#     except:\n",
    "#         test_data = np.asarray(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(test_data, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(prediction, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = 100\n",
    "nb = 0 \n",
    "for pred, file in zip(prediction, glob.glob(\"*.png\")):\n",
    "    \n",
    "    if pred[1] > .9:\n",
    "        Image.open(file).show()\n",
    "        nb += 1\n",
    "    if nb == x:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python deep learning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
